{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec925dce",
   "metadata": {},
   "source": [
    "\n",
    "# MDC Dataset Linking Baseline\n",
    "\n",
    "This notebook implements a two-stage pipeline:\n",
    "\n",
    "1. **Stage A** – candidate extraction from XML/PDF.\n",
    "2. **Stage B** – encoder based classification of extracted candidates.\n",
    "\n",
    "The goal is to locate dataset identifiers such as DOI and accessions within scientific articles and classify their roles (Primary, Secondary, or Noise).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f65f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "env = {\n",
    "    'gpus': [],\n",
    "    'cpu_count': os.cpu_count(),\n",
    "    'ram_gb': round(psutil.virtual_memory().total / 1024 ** 3, 2),\n",
    "}\n",
    "if torch.cuda.is_available():\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        prop = torch.cuda.get_device_properties(i)\n",
    "        env['gpus'].append({\n",
    "            'name': prop.name,\n",
    "            'total_mem_gb': round(prop.total_memory / 1024 ** 3, 2),\n",
    "        })\n",
    "\n",
    "print(env)\n",
    "Path('/kaggle/working').mkdir(parents=True, exist_ok=True)\n",
    "with open('/kaggle/working/ENV.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(env, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81055192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Offline installation of required packages\n",
    "wheels_dir = '/kaggle/input/wheels/'\n",
    "packages = [\n",
    "    'transformers',\n",
    "    'tokenizers',\n",
    "    'lxml',\n",
    "    'pymupdf',\n",
    "    'pyarrow',\n",
    "    'regex',\n",
    "    'tqdm',\n",
    "    'accelerate',\n",
    "]\n",
    "!pip install --no-index --find-links={wheels_dir} {' '.join(packages)}\n",
    "\n",
    "import lxml\n",
    "import pyarrow\n",
    "import regex\n",
    "import tokenizers\n",
    "import tqdm\n",
    "import transformers\n",
    "\n",
    "try:\n",
    "    import accelerate  # optional\n",
    "    print('accelerate', accelerate.__version__)\n",
    "except Exception:\n",
    "    print('accelerate not available')\n",
    "\n",
    "print('transformers', transformers.__version__)\n",
    "print('tokenizers', tokenizers.__version__)\n",
    "print('lxml', lxml.__version__)\n",
    "print('pyarrow', pyarrow.__version__)\n",
    "print('regex', regex.__version__)\n",
    "print('tqdm', tqdm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ace6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "CFG = {\n",
    "    'train_xml_dir': Path('/kaggle/input/train_xml'),\n",
    "    'train_pdf_dir': Path('/kaggle/input/train_pdf'),\n",
    "    'test_xml_dir': Path('/kaggle/input/test_xml'),\n",
    "    'test_pdf_dir': Path('/kaggle/input/test_pdf'),\n",
    "    'labels_path': Path('/kaggle/input/labels.csv'),\n",
    "    'sample_sub_path': Path('/kaggle/input/sample_submission.csv'),\n",
    "    'max_len': 384,\n",
    "    'batch_size': 8,\n",
    "    'num_workers': 4,\n",
    "    'use_two_gpus': True,\n",
    "    'cache_dir': Path('/kaggle/working/cache'),\n",
    "}\n",
    "CFG['cache_dir'].mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0577e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import contextlib\n",
    "import logging\n",
    "import time\n",
    "from typing import Iterator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def timer(name: str) -> Iterator[None]:\n",
    "    start = time.time()\n",
    "    yield\n",
    "    end = time.time()\n",
    "    print(f'{name}: {end - start:.2f}s')\n",
    "\n",
    "\n",
    "def get_logger() -> logging.Logger:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "    return logging.getLogger('mdc')\n",
    "\n",
    "\n",
    "def save_parquet(df: pd.DataFrame, path: Path) -> None:\n",
    "    tmp = path.with_suffix('.tmp')\n",
    "    df.to_parquet(tmp, index=False)\n",
    "    tmp.replace(path)\n",
    "\n",
    "\n",
    "def load_parquet(path: Path) -> pd.DataFrame:\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def report_memory() -> None:\n",
    "    vm = psutil.virtual_memory()\n",
    "    used = (vm.total - vm.available) / 1024 ** 3\n",
    "    total = vm.total / 1024 ** 3\n",
    "    print(f'RAM used: {used:.2f}GB / {total:.2f}GB')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f682ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "DOI_PREFIX = 'https://doi.org/'\n",
    "SECTION_MAP = {\n",
    "    'data availability': 'Data Availability',\n",
    "    'availability of data': 'Data Availability',\n",
    "}\n",
    "\n",
    "def normalize_doi(text: str) -> str:\n",
    "    if not text:\n",
    "        return ''\n",
    "    doi = text.strip().lower().rstrip('.;/')\n",
    "    if doi.startswith(DOI_PREFIX):\n",
    "        doi = doi[len(DOI_PREFIX):]\n",
    "    if doi.startswith('10.'):\n",
    "        return f'{DOI_PREFIX}{doi}'\n",
    "    return doi\n",
    "\n",
    "def normalize_id(text: str) -> str:\n",
    "    return text.strip()\n",
    "\n",
    "def normalize_section(name: str) -> str:\n",
    "    return SECTION_MAP.get(name.strip().lower(), name.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94af8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def build_index(xml_dir: Path, pdf_dir: Path, out_path: Path) -> dict:\n",
    "    index = {}\n",
    "    for path in xml_dir.glob('*.xml'):\n",
    "        article_id = path.stem\n",
    "        index.setdefault(article_id, {})['xml_path'] = str(path)\n",
    "    for path in pdf_dir.glob('*.pdf'):\n",
    "        article_id = path.stem\n",
    "        index.setdefault(article_id, {})['pdf_path'] = str(path)\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(index, f)\n",
    "    return index\n",
    "\n",
    "index_train = build_index(\n",
    "    CFG['train_xml_dir'], CFG['train_pdf_dir'], CFG['cache_dir'] / 'index_train.pkl'\n",
    ")\n",
    "index_test = build_index(\n",
    "    CFG['test_xml_dir'], CFG['test_pdf_dir'], CFG['cache_dir'] / 'index_test.pkl'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38672c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from lxml import etree\n",
    "\n",
    "\n",
    "def parse_xml(xml_path: str) -> list[dict]:\n",
    "    \"\"\"Parse JATS/PubMed XML and return section windows.\n",
    "\n",
    "    This is a simplified placeholder implementation.\n",
    "    \"\"\"\n",
    "    tree = etree.parse(xml_path)\n",
    "    # TODO: extract sections and sliding windows\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ab8d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "\n",
    "def parse_pdf(pdf_path: str) -> list[dict]:\n",
    "    \"\"\"Fallback PDF text extractor.\n",
    "\n",
    "    Only used when XML is missing.\n",
    "    \"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    text = '\\n'.join(page.get_text() for page in doc)\n",
    "    doc.close()\n",
    "    # TODO: clean text and split into windows\n",
    "    return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8cdf6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "DOI_RE = re.compile(r'10\\.\\d{4,9}/[-._;()/:A-Za-z0-9]+')\n",
    "ACC_RE = re.compile(r'PRJNA\\d+|PRJEB\\d+|SRR\\d+|SRX\\d+|SRA\\d+|GSE\\d+|GSM\\d+|PDB[0-9A-Za-z]{4}')\n",
    "\n",
    "\n",
    "def extract_candidates(windows: list[dict]) -> list[dict]:\n",
    "    \"\"\"Run regex patterns over text windows.\"\"\"\n",
    "    candidates = []\n",
    "    for idx, win in enumerate(windows):\n",
    "        text = win.get('window_text', '')\n",
    "        for match in DOI_RE.findall(text):\n",
    "            candidates.append({\n",
    "                'raw_id': match,\n",
    "                'id_type': 'doi',\n",
    "                'section': win.get('section'),\n",
    "                'window_idx': idx,\n",
    "            })\n",
    "        for match in ACC_RE.findall(text):\n",
    "            candidates.append({\n",
    "                'raw_id': match,\n",
    "                'id_type': 'accession',\n",
    "                'section': win.get('section'),\n",
    "                'window_idx': idx,\n",
    "            })\n",
    "    return candidates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7237f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def normalize_candidates(article_id: str, candidates: list[dict]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for c in candidates:\n",
    "        dataset_id = (\n",
    "            normalize_doi(c['raw_id']) if c['id_type'] == 'doi' else normalize_id(c['raw_id'])\n",
    "        )\n",
    "        rows.append({\n",
    "            'article_id': article_id,\n",
    "            'raw_id': c['raw_id'],\n",
    "            'dataset_id': dataset_id,\n",
    "            'id_type': c['id_type'],\n",
    "            'section': c.get('section'),\n",
    "            'window_idx': c.get('window_idx'),\n",
    "        })\n",
    "    df = pd.DataFrame(rows).drop_duplicates([\n",
    "        'article_id',\n",
    "        'dataset_id',\n",
    "        'window_idx',\n",
    "    ])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2fbe4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def candidate_report(df: pd.DataFrame, labels_path: Path) -> pd.DataFrame:\n",
    "    labels = pd.read_csv(labels_path)\n",
    "    merged = df.merge(labels, on=['article_id', 'dataset_id'], how='left')\n",
    "    recall = merged['dataset_id_y'].notna().mean()\n",
    "    print(f'Upper-bound recall: {recall:.2%}')\n",
    "    return merged\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca614c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a048d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 15 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 16 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47509610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a25ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 18 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a69730e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 19 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4423da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 20 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d3afd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 21 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0079781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 22 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9aab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 23 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94da1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 24 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d88c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 25 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a786f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 26 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f8e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 27 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7a3648",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 28 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b373460c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 29 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc60b502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 30 placeholder\n",
    "# TODO: implement stage according to plan"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
